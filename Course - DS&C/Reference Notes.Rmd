---
title: "Course Notes"
author: "pdwessel"
date: "September 17, 2016"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```
#Week 1

###Checking and creating directories

- file.exists("directory name") will check to see if the directory exists
- dir.create("directory name") will create a directory if it doesn't exist

```{r echo = TRUE, results = FALSE} 
if (!file.exists("data")) {
        dir.create("data")
} 
```

Getting data from the internet - download.file()
- downloads file from the internet
- improves reprducability

DateDownloaded <- date()

###Reading in an xlsx file

``` {r echo = TRUE}
library(xlsx)
cameraData <- read.xlsx("./data/cameras.xlsx", sheetIndex = 1, header = TRUE)
head(cameraData)
```

The write.xlsx function will write out an excel file
The XLConnect package has more options for writing and manipulating Excel files.



##XML

Extracting XML is the basis for most web scraping
- tags correspond to general labels
        - Start tags <section>
        - End tags </section>
        - Empty tags <line-break />
- Elements are specific examples of tags
        - <Greeting> Hello, world </Greeting>
- Attributes are components of the label
        - <img scr="x.jpg" alt="ins"/>
        
``` {r echo = TRUE}
library(XML)
fileurl <- "http://www.w3schools.com/xml/simple.xml"
doc <- xmlTreeParse(fileurl, useInternalNodes = TRUE)
rootNode <- xmlRoot(doc)
xmlName(rootNode)
```
    

``` {r}
rootNode[[1]]
```

``` {r}
rootNode[[1]][[1]]
```

``` {r}
xmlSApply(rootNode, xmlValue)
```

XPath, is another language to learn to access XML code

``` {r}
xpathSApply(rootNode, "//name", xmlValue)
```

##JSON

Javascript Object Notation

* Lightweight data storage
* Common format for data from application programming interfaces
* Similar structure to Xml but different syntax/format
* Data stored as
        + Numbers (double)
        + Strings (double quoted)
        + Boolean (true or false)
        + Array (ordered, comma seperated enclosed in square brakets [])
        + Object (unordered, comma seperated collection of key:value pairs in curley brackets{})

Example
``` {r}
library(jsonlite)
jsonData <- fromJSON("https://api.github.com/users/jtleek/repos")
names(jsonData)
```

``` {r}
myjson <- toJSON(iris, pretty=TRUE)
cat(myjson)
```


## Using data.table

Inherets from data.frame
* All function that accept data.frame work on data.table
* Written in C so much faster
* Much, much faster at subsetting, group and updating

``` {r}
library(data.table)
DF = data.frame(x=rnorm(9), y = rep(c("a", "b", "c"), each =3), z= rnorm(9))
head(DF,3)
```

``` {r}
DT = data.table(x=rnorm(9), y= rep(c("A","b", "c"), each = 3), z= rnorm(9))
head(DT, 3)
```

Column subsetting in data.table
* The subsetting function is modified for a data.table
* The argument you pass after the comma is called an expression
* In R an expression is a collection of statements enclosed in curley brackets

``` {r}
DT[,list(mean(x), sum(z))]
```

``` {r}
DT[,w:=z^2]
```

need to explicitly use the copy funciton to copy over a data.table to new variable

``` {r}
DT[,m:= { tmp <- x+z; log2(tmp+5)}]
```

Special Variables

.N An integer, length 1, containing the number r

``` {r}
set.seed(123)
DT2 <- data.table(x=sample(letters[1:3], 1E5, TRUE))
DT2[, .N, by=x]
```

Keys

``` {r}
DT3 <- data.table(x=rep(c("a","b","c"), each = 100), y=rnorm(300))
setkey(DT3,x)
DT3['a']
```

``` {r}
DT4 <- data.table(x=c('a','a','b','dt1'), y=1:4)
DT5 <- data.table(x=c('a','b','dt2'), z=5:7)
setkey(DT4,x); setkey(DT5,x)
merge(DT4,DT5)
```

Fast Reading

``` {r}
big_df <- data.frame(x=rnorm(1E6), y=rnorm(1E6))
file <- tempfile()
write.table(big_df, file=file, row.names = FALSE, col.names = FALSE, sep = "\t", quote = FALSE)

system.time(fread(file))

system.time(read.table(file, header = TRUE, sep = "\t"))

```

The latest development version contains new function like **melt** and **dcast**

 

 system.time(replicate(1000, DT[,mean(pwgtp15),by=SEX]))
 system.time(replicate(1000,tapply(DT$pwgtp15,DT$SEX,mean)))
 system.time(replicate(1000,sapply(split(DT$pwgtp15, DT$SEX),mean)))

 
# Week 2

##mySQL

``` {r}

library(RMySQL)
ucscDb <- dbConnect(MySQL(), user="genome",host="genome-mysql.cse.ucsc.edu")
result <- dbGetQuery(ucscDb, "show databases;"); dbDisconnect(ucscDb);
```

"dbConnect" can be used to connect to other types of databases as well

``` {r}
hg19 <- dbConnect(MySQL(), user="genome", db="hg19", host ="genome-mysql.cse.ucsc.edu")
allTables <- dbListTables(hg19)
length(allTables)
```

```{r}
dbListFields(hg19,"affyU133Plus2")
dbGetQuery(hg19, "select count(*) from affyU133Plus2")
```

``` {r}
affyData <- dbReadTable(hg19, "affyU133Plus2")
head(affyData)
```

``` {r}
query <- dbSendQuery(hg19, "select * from affyU133Plus2 where misMatches between 1 and 3")
affyMis <- fetch(query); quantile(affyMis$misMatches)
```

```{r}
dbClearResult(query)
dbDisconnect(hg19)
```
Remember to disconnect from publicly facing DB's when done. Only pull data, do not push/delete or modify public DBs.

More info can be found at:
http://www.r-bloggers.com/mysql-and-r/
                
##HDF5

*Used for Storing large data sets
*Supports storing a range of data types

```{r}
source("http://bioconductor.org/biocLite.R")
biocLite("rhdf5")

library(rhdf5)
created = h5createFile("example.h5")
created

```


```{r}
created = h5createGroup("example.h5","foo")
created = h5createGroup("example.h5","baa")
created = h5createGroup("example.h5","foo/foobaa")
h5ls("example.h5")
```

```{r}
A = matrix(1:10,nr=5,nc=2)
h5write(A, "example.h5","foo/A")
B = array(seq(0.1,2.00,by=0.1), dim=c(5,2,2))
attr(B, "scale") <- "liter"
h5write(B, "example.h5","foo/foobaa/B")
h5ls("example.h5")
```


```{r}
df = data.frame(1L:5L,seq(0,1,length.out=5), c("ab", "cde", "fghi", "a", "s"), stringsAsFactors = FALSE)
h5write(df, "example.h5","df")
h5ls("example.h5")
H5close()
```

```{r}
readA = h5read("example.h5", "foo/A")
```


```{r}
h5write(c(12,13,14),"example.h5","foo/A",index=list(1:3,1))
h5read("example.h5", "foo/A")
```

##Webscraping

Programatically extracting data from the HTML code of websites

Example: Google Scholar

```{r}
con = url("http://scholar.google.com/citations?user=HI-I6C0AAAAJ&hl=en")
htmlCode = readLines(con)
close(con)
htmlCode
```


```{r}
library(XML)
url <- "http://scholar.google.com/citations?user=HI-I6C0AAAAJ&hl=en"
html <- htmlTreeParse(url, useInternalNodes=TRUE)

xpathSApply(html, "//title", xmlValue)

xpathSApply(html, "//td[@id='col-citedby']", xmlValue)
```


```{r}
library(httr); html2 = GET(url)
content2 = content(html2, as="text")
parsedHtml = htmlParse(content2, asText=TRUE)
xpathSApply(parsedHtml, "//title", xmlValue)
```

Using handles

```{r}
google = handle("http://google.com")
pg1 = GET(handle = google, path="/")
pg2 = GET(handle = google, path="search")
```

Lots of good examples and tutorials for web scraping on R blogger.

##API's

Application Programming Interface

Use the httr package. Will normally have to create an account with each API

```{r echo = FALSE}
myapp = oauth_app("twitter", key = "OA82Rt05pjE5dB9drifqd3Q3G", secret = "4QuVqZ7fduB1Kx28WBPc29W77aVL34xy4GiY90ZwDZ3Cr3uOqE")

sig = sign_oauth1.0(myapp, token="951722202-TXfBI4IahowpROAV840GAkWtKZy31m6eZYUyiOG5",
token_secret = "MEEKcJPw3we18QpojRnaGIHq5Kpgq13nigzm4w0Huf03H")

homeTL = GET("https://api.twitter.com/1.1/statuses/home_timeline.json", sig)
```


```{r}
library(jsonlite)
json1 = content(homeTL)
json2 = jsonlite::fromJSON(toJSON(json1))
json2[1,1:4]
```

demos for Api access on github.

##Other Sources

*search for "source R package"
*remember to close connections
*foreign package is useful to read other statistical data software formats
*can read images, gis data, music data
