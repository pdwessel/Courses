x <- c(0.8, 0.47, 0.51, 0.73, 0.36, 0.58, 0.57, 0.85, 0.44, 0.42)
y <- c(1.39, 0.72, 1.55, 0.48, 1.19, -1.59, 1.23, -0.65, 1.49, 0.05)
lm(y~x-1)
data(mtcars)
lm(mpg~weight, mtcars)
head(mtcars)
lm(mpg~wt, mtcars)
x <- c(8.58, 10.46, 9.01, 9.64, 8.86)
x_nor <- (x - mean(x))/sd(x)
x_nor
x_nor <- (x - mean(x))/var(x)
x_nor
x <- c(0.8, 0.47, 0.51, 0.73, 0.36, 0.58, 0.57, 0.85, 0.44, 0.42)
y <- c(1.39, 0.72, 1.55, 0.48, 1.19, -1.59, 1.23, -0.65, 1.49, 0.05)
lm(y~x)
x <- c(0.8, 0.47, 0.51, 0.73, 0.36, 0.58, 0.57, 0.85, 0.44, 0.42)
mean(x)
mse <- function(mu){sum(w*(x-mu)^2)}
mse(1.077)
x <- c(0.18, -1.54, 0.42, 0.95)
w <- c(2, 1, 3, 1)
mse(1.077)
mse(.0025)
mse(.1471)
mse(.3)
sum(x*w)/sum(w)
library(UsingR); library(ggplot2); data(diamond)
g1 <- ggplot(diamond, aes(x = carat, y = price))
g1 <- g1 + xlab("Mass (carats)")  + ylab("Price ($)")
g1 <- g1 + geom_point(size = 6, colour = "black", alpha = 0.2)
g1 <- g1 + geom_point(size = 5, colour = "bllu", alpha = 0.2)
g1 <- g1 + geom_smooth(method = "lm", colour = "black")
g1
library(UsingR); library(ggplot2); data(diamond)
g1 <- ggplot(diamond, aes(x = carat, y = price))
g1 <- g1 + xlab("Mass (carats)")  + ylab("Price ($)")
g1 <- g1 + geom_point(size = 3, colour = "black", alpha = 0.2)
g1 <- g1 + geom_point(size = 2, colour = "blue", alpha = 0.2)
g1 <- g1 + geom_smooth(method = "lm", colour = "black")
g1
swirl()
fit <- lm(child~parent, galton)
sqrt(sum(fit$residuals)^2/(n-2))
sqrt(sum(fit$residuals^2)/(n-2))
summary(fit)$sigma
sqrt(deviance(fit)/(n-2))
mu <- mean(galton$child)
sTot <- sum(child-mu)^2
sTot <- sum(galton$child-mu)^2
sTot <- sum((galton$child-mu)^2)
sRes <- deviance(fit)
sRes/sTot
1-sRes/sTot
summary(fit)$r.squared
cor(galton$child, galton$parent)^2
ones <- rep(1,nrow(galton))
lm(child~ones + parent -1, galton)
lm(child~parent,galton)
lm(child,1,galton)
lm(child~1,galton)
head(trees)
fit <- lm(Volume~Girth + Height + Constant -1, trees)
trees2 <- eliminate("Girth", trees)
head(trees2)
fit2 <- lm(Volume ~ Height + Constant -1, trees2)
lappy(list(fit,fit2), coef)
lapply(list(fit,fit2), coef)
all <- lm(Fertility~., swiss)
summary(all)
summary(lm(Fertility~Agricultre,swiss))
summary(lm(Fertility~Agriculture,swiss))
cor(Examination, Education)
with(swiss, cor(Examination, Education))
cor(swiss$Examination, swiss$Education))
cor(swiss$Examination, swiss$Education)
cor(swiss$Agriculture, swiss$Education)
makelms()
ec <- swiss$Examination = swiss$Catholic
ec <- swiss$Examination + swiss$Catholic
efit <- lm(Fertility~. + ec, swiss)
all$coefficients - efit$coefficients
?mtcars
library(swirl)
swirl()
fit <- lm(y~x, out2)
plot(fit, which=1)
fitno <- lm(y~x, out2[-1,])
plot(fitno, which=1)
coef(fit) - coef(fitno)
head(dfbeta(fit))
resno <- out2[1,"y"] - predict(fitno, out2[1,])
1 - resid(fit)[1]/resno
head(ahtvalues(fit))
head(hatvalues(fit))
sigma <- sqrt(sum(resid(fit)^2)/fit$df.residual)
rstd <- resid(fit)/(sigma*(1 - sqrt(hatvalues(fit)))
)
rstd <- resid(fit)/(sigma*(1 - sqrt(hatvalues(fit))))
rstd <- resid(fit)/(sigma * sqrt(1 - hatvalues(fit))))
rstd <- resid(fit)/(sigma * sqrt(1 - hatvalues(fit)))
head(cbind(rstd, rstandard(fit)))
plot(fit, which = 3)
plot(fit, which = 2)
sigma1 <- sqrt(resid(fitno)/fit$df.residual))
sigma1 <- sqrt(resid(fitno)/fit$df.residual)
sigma1 <- sqrt(deviance(fitno)/fitno$df.residual)
resid(fit)[1]/(sqrt(1-hatvalues(fit)[1])*sigma1)
head(rstudent(fit))
dy <- predict(fitno, out2) - predict(fit, out2)
dy/(2*sigma^2)
sum(dy^2)/(2*sigma^2)
plot(fit, which = 5)
library(datsets)
library(dataset)
library(datasets)
data(mtcars)
head(mtcars)
fit1 <- lm(mpg~factor(cyl) + wt, mtcars)
summary(fit1)
fit2 <- lm(mpg~factor(cyl), mtcars)
summary(fit2)
fit3 <- lm(mpg~factor(cyl) * wt, mtcars)
summary(fit3)
anova(fit, fit3)
install.packages(epicalc)
install.packages("epicalc")
?lrtest
install.packages("lmtest")
anova(fit1, fit3)
fit4 <- lm(mpg~ I(wt * 0.5) + factor(cyl), data = mtcars)
summary(fit4)
x <- c(0.586, 0.166, -0.042, -0.614, 11.72)
y <- c(0.549, -0.026, -0.127, -0.751, 1.344)
fit5(y~x)
fit5 <- lm(y~x)
hatvalues(fit5)
x <- c(0.586, 0.166, -0.042, -0.614, 11.72)
y <- c(0.549, -0.026, -0.127, -0.751, 1.344)
fit6 <- lm(y~x)
dfbeta(fit6)
influence.measures(fit6)
x <- seq(-10, 10, length = 1000)
manipulate(plot(x, exp(beta0 + beta1 * x)/(1 + exp(beta0 + beta1*x)), typl = "l", lwd = 3, frame = FALSE),
beta0 = slider(-2,2, step = 0.1, initial = 2), beta1 = slider(-2,2, step = 0.1, initial = 0))
library(manipulate)
x <- seq(-10, 10, length = 1000)
manipulate(plot(x, exp(beta0 + beta1 * x)/(1 + exp(beta0 + beta1*x)), typl = "l", lwd = 3, frame = FALSE),
beta0 = slider(-2,2, step = 0.1, initial = 2), beta1 = slider(-2,2, step = 0.1, initial = 0))
library(swirl)
swirl()
rpg1()
rgp1 <- function(){
print("Processing. Please wait.")
# number of samples per simulation
n <- 100
# number of simulations
nosim <- 1000
# set seed for reproducibility
set.seed(4321)
# Point A
x1 <- rnorm(n)
x2 <- rnorm(n)
x3 <- rnorm(n)
# Point B
betas <- sapply(1 : nosim, function(i)makelms(x1, x2, x3))
roun
rgp1 <- function(){
print("Processing. Please wait.")
# number of samples per simulation
n <- 100
# number of simulations
nosim <- 1000
# set seed for reproducibility
set.seed(4321)
# Point A
x1 <- rnorm(n)
x2 <- rnorm(n)
x3 <- rnorm(n)
# Point B
betas <- sapply(1 : nosim, function(i)makelms(x1, x2, x3))
round(apply(betas, 1, var), 5)
}
rgp1()
rgp2()
head(swiss)
mdl <- lm(Fertility ~ ., data = swiss)
vif(mdl)
mdl2 <- lm(Fertility ~ . -Examination, data = swiss)
vif(mdl2)
x1c <- simbas()
x1c <- simbias()
apply(x1c,1,mean)
fit1 <-  lm(Fertility ~ Agriculture, data = swiss)
fit3 <- lm(Fertility ~ Agriculture + Examination + Education, data = swiss)
anova(fit1, fit3)
deviance(fit3)
d <- deviance(fit3)/43
n <- (deviance(fit1)-deviance(fit3))/2
n/d
pf(n/d, 2, 43, lower.tail = FALSE)
shapiro.test(fit3$residuals)
anova(fit, fit3, fit5, fit6)
anova(fit1, fit3, fit5, fit6)
library(swirl)
swirl()
ravenData
mdl <- glm(ravenWinNum~ravenScore, family = "binomial", data = ravenData)
lodds <- predict(mdl, data.frame(ravenScore=c(0,3,6)))
exp(lodds)/(1+exp(lodds))
summary(mdl)
confint(mdl(
confint(mdl)
exp(confint(mdl))
anova(mdl)
qchisq(.95, 1)
library(swirl)
swirl()
var(rpois(1000,50))
head(hits)
class(hits[,'dates'])
class(hits[,'date'])
as.integer(head(hits[,'dates']))
as.integer(head(hits[,'date']))
mdl <- glm(visits~date, poisson, hits)
summary(mdl)
confint(mdl,'date')
exp(confint(mdl,'date'))
which.max((hits[,'visits'])
)
hits[704,]
lambda <- mdl$fitted.values[704]
qpois(.95,lambda)
mdl2 <- glm(visits~date, poisson, hits, offset = log(visits + 1))
mdl2 <- glm(formula = simplystats~ data, family = poisson, data = hits, offset = log(visits + 1))
mdl2 <- glm(formula = simplystats~ date, family = poisson, data = hits, offset = log(visits + 1))
qpois(.95, mdl2$fitted.values[704])
library(MASS)
?shuttle
fit1 <- glm(use ~ wind, family = 'binomial', data = shuttle)
?glm
exp(coef(fit1))
fit2 <- glm(use ~ wind + magn, family = 'binomial', data = shuttle)
exp(coef(fit2))
fit3 <- glm((use -1) ~ wind, family = 'binomial', data = shuttle)
use2 <- shuttle[,'use'] -1
use2 <- as.factor(-1*as.numeric(shuttle[,'use']) - 1))
use2 <- as.factor(-1*(as.numeric(shuttle[,'use']) - 1))
fit3 <- glm(use2 ~ wind, family = 'binomial', data = shuttle)
coef(fit3)
coef(fit1)
head(InsectSprays)
fit4 <- glm(count ~ spray, family = poisson, data = InsectSprays)
exp(coef(fit4))
exp(coef(fit4))[2]
1/exp(coef(fit4))[2]
load("~/Data Science/Courses/Course - DS&C/.RData")
install.packages('kernlab')
library(kernlab)
data(spam)
head(spam)
install.packages('caret')
library(caret)
library(kernlab)
library(caret)
args(train.default)
install.packages('ISLR')
library(ISLR); library(ggplot2); library(caret)
data(wage)
date(Wage)
summary(Wage)
inTrain <- createDataPartition(y=Wage$wage, p=0.7, list=FALSE)
training <- Wage[inTrain,]
testing <- Wage[-inTrain,]
dim(training);dim(testing)
featurePlot(x=training[,c("age","education",  "jobclass")],y=training$wage,plot="pairs")
library(caret); library(kernlab); data(spam)
inTrain <- createDataPartition(y=spam$type, p=0.75, list=FALSE)
training <- spam[inTrain,]
testing <- spam[-inTrain,]
M <- abs(cor(training[-58]))
diag(M) <- 0
which(M>0.8, arr.ind = T)
library(AppliedPredictiveModeling)
data(AlzheimerDisease)
install.packages("AppliedPredictiveModeling")
library(AppliedPredictiveModeling)
library(AppliedPredictiveModeling)
data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
trainIndex = createDataPartition(diagnosis, p = 0.50,list=FALSE)
training = adData[trainIndex,]
testing = adData[-trainIndex,]
dim(training)
dim(testing)
trainIndex = createDataPartition(diagnosis,p=0.5,list=FALSE)
training = adData[trainIndex,]
testing = adData[-trainIndex,]
dim(training)
dim(testing)
?createDataPartition
library(AppliedPredictiveModeling)
data(concrete)
library(caret)
set.seed(1000)
inTrain = createDataPartition(mixtures$CompressiveStrength, p = 3/4)[[1]]
training = mixtures[ inTrain,]
testing = mixtures[-inTrain,]
library(AppliedPredictiveModeling)
data(concrete)
library(caret)
set.seed(1000)
inTrain = createDataPartition(mixtures$CompressiveStrength, p = 3/4)[[1]]
training = mixtures[ inTrain,]
testing = mixtures[-inTrain,]
hist(mixtures$Superplasticizer)
hist(log(mixtures$Superplasticizer))
hist(log(mixtures$Superplasticizer+1))
library(caret)
library(AppliedPredictiveModeling)
set.seed(3433)data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]training = adData[ inTrain,]
testing = adData[-inTrain,]
library(caret)
library(AppliedPredictiveModeling)
set.seed(3433);data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]training = adData[ inTrain,]
testing = adData[-inTrain,]
library(caret)
library(AppliedPredictiveModeling)
set.seed(3433)data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
library(caret)
library(AppliedPredictiveModeling)
set.seed(3433);data(AlzheimerDisease)
adData = data.frame(diagnosis,predictors)
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]]training = adData[inTrain,]
inTrain = createDataPartition(adData$diagnosis, p = 3/4)[[1]];training = adData[inTrain,]
testing = adData[-inTrain,]
head(training)
?preProcess
head(training[grep(^IL),])
?grep
head(training[grep(IL),])
head(training[grep("IL", names(training)),])
head(training[grep("^IL", names(training)),])
names(training)
grepl("^IL",names(training))
head(training[grepl("^IL",names(training)),])
head(training[grepl("^IL",names(training)),])
s <- grepl("^IL",names(training))
min <- subset(training,s)
head(min(
head(min)
?subset
min <- subset(training,select = s)
head(min)
?preProcess
preProcess(min, method = "pca", thresh = 0.8)
library(caret)
install.packages(c("acepack", "assertthat", "BH", "chron", "colorspace", "curl", "data.table", "DBI", "digest", "dtplyr", "foreign", "formatR", "git2r", "Hmisc", "hms", "htmltools", "jsonlite", "knitr", "lattice", "lme4", "markdown", "Matrix", "memoise", "mgcv", "openssl", "pbdZMQ", "pbkrtest", "proto", "quantmod", "quantreg", "R6", "Rcpp", "RcppEigen", "readr", "repr", "reshape2", "rgdal", "rmarkdown", "RMySQL", "rsconnect", "RSQLite", "SparseM", "stringi", "stringr", "survival", "swirl", "tibble", "tidyr", "XML", "yaml", "zoo"))
install.packages('AppliedPredictiveModeling')
library(AppliedPredictiveModeling)
library(caret)
data("segmentationOriginal")
inTrain <- createDataPartition(y=segmentationOriginal$Case, p=0.8, list=FALSE)
training <- segmentationOriginal[inTrain,]
testing <- segmentationOriginal[-inTrain,]
modFit <- train(Case~.,method="rpart", data=training)
install.packages('caret', dependencies = TRUE)
install.packages("caret", dependencies = TRUE)
library(AppliedPredictiveModeling)
library(caret)
data("segmentationOriginal")
inTrain <- createDataPartition(y=segmentationOriginal$Case, p=0.8, list=FALSE)
training <- segmentationOriginal[inTrain,]
testing <- segmentationOriginal[-inTrain,]
modFit <- train(Case~.,method="rpart", data=training)
head(segmentationOriginal)
newdata <- data.frame(TotalIntench2 = 23,000; FiberWidthCh1 = 10; PerimStatusCh1=2)
newdata <- data.frame(TotalIntench2 = 23,000, FiberWidthCh1 = 10, PerimStatusCh1=2)
predict(modFit, newdata)
d1 <- data.frame(TotalIntench2 = 23,000, FiberWidthCh1 = 10, PerimStatusCh1=2)
head(d1)
d1 <- data.frame(TotalIntench2 = 23000, FiberWidthCh1 = 10, PerimStatusCh1=2)
d1
predict(modFit,d1)
training <- segmentationOriginal[,Case=="training"]
training <- segmentationOriginal[,segmentationOriginal$Case=="training"]
testing <- segmentationOriginal[,segmentationOriginal$Case=="testing"]
set.seed(125)
modFit <- train(Case~., data=training, method="rpart")
modFit <- train(Case~., method="rpart", data=training)
modFit <- train(Case ~ ., method="rpart", data=training)
?subset
training <- subset(segmentationOriginal, segmentationOriginal$Case=="training")
training <- subset(segmentationOriginal, segmentationOriginal$Case=="train")
training <- subset(segmentationOriginal, segmentationOriginal$Case="train")
training <- subset(segmentationOriginal, segmentationOriginal$Case=="Train")
testing <- subset(segmentationOriginal, segmentationOriginal$Case=="Test")
modFit <- Train(Case~.,method="rpart", data=training)
modFit <- train(Case~.,method="rpart", data=training)
modFit <- train(Class~.,method="rpart", data=training)
predict(modFit,d1)
d1 <- testing[1,]
d1 <- NA
d1 <- testing[1,]
head(d1)
empty <- zeros(119)
?rep
empty <- rep(0,119)
names(empty) <- names(d1)
head(names)
head(empty)
empty <- as.data.frame(empty)
for(i in 1:119){d1[,i] = NA}
head(d1)
d1$TotalIntenCh2 = 23000
d1
d1$FiberWidthCh1 = 10
d1$PerimStatusCh1 = 2
predict(modFit,d1)
for(i in 1:119){d1[,i] = 0}
d1$TotalIntenCh2 = 23000
d1$FiberWidthCh1 = 10
d1$PerimStatusCh1 = 2
predict(modFit,d1)
predict(modFit, newdata = data.frame(TotalIntench2 = 23000,FiberWidthCh1 = 10, PerimStatusCh1=2))
print(modFit$finalModel)
library(pgmm)
install.packages('pgmm')
library(pgmm)
data(olive)
olive = olive[,-1]
mdl2 <- train(Area~.,method="rpart",data=olive)
newdata = as.data.frame(t(colMeans(olive))
)
newdata = as.data.frame(t(colMeans(olive)))
predict(mdl2,newdata)
head(olive)
tail(olive)
install.packages("ElemStatLearn")
library(ElemStatLearn)
data("SAheart")
set.seed(8484)
train = sample(1:dim(SAheart)[1], size = dim(SAheart)[1]/2,replace=F)
trainSA = SAheart[train,]
testSA = SAheart[-train,]
set.seed(13234)
head(SAheart)
set.seed(13234)
mdl4 <- train(chd~age+alcohol+obesity+tobacco+typea+ldl,method="glm", family="binomial", data=trainSA, list=FALSE)
?train
set.seed(13234)
mdl4 <- train(chd~age+alcohol+obesity+tobacco+typea+ldl,method="glm", family="binomial", data=trainSA)
trainP <- predict(mdl4,trainSA)
testP <- predict(mdl4,testSA)
missClass = function(values,prediction){sum(((prediction > 0.5)*1) != values)/length(values)}
missClass(trainSA$chd,trainP)
missClass(testSA$chd,testP)
data("vowel.train")
data("vowel.test")
vowel.test$y <- as.factor(vowel.test$y)
vowel.train$y <- as.factor(vowel.train$y)
set.seed(33833)
?randomForest
install.packages("randomForest")
library(randomForest)
?randomForest
set.seed(33833)
mdl5 <- randomForest(y~.,data=vowel.train)
print(mdl5)
?varImp
varImp(mdl5)
setwd('Courses')
setwd('Course - Intro ML')
knitr::opts_chunk$set(echo = TRUE)
#load all relevant libraries
library(caret); library(kernlab); library(ggplot2)
#load all relevant libraries
library(caret); library(kernlab); library(ggplot2)
#Create a file to download the data to
if (!dir.exists("data")) {
dir.create("data")
}
#Download the data
trainurl <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-training.csv"
testurl <- "https://d396qusza40orc.cloudfront.net/predmachlearn/pml-testing.csv"
trainfile <- "data/train.csv"
testfile <- "data/test.csv"
if (!file.exists(trainfile)) {
download.file(trainurl, trainfile)
}
if (!file.exists(testfile)) {
download.file(testurl, testfile)
}
RawData <- read.csv(trainfile)
naVariables <- sapply(RawData, function(y) sum(length(which(is.na(y)))))
remove1 <- naVariables > 0 #All the variables with NA values actually have 19216 missing values or 98% of their inputs.
remove <- !remove1&!remove2
naVariables <- sapply(RawData, function(y) sum(length(which(is.na(y)))))
remove1 <- naVariables > 0 #All the variables with NA values actually have 19216 missing values or 98% of their inputs.
noInfoVariables <- c("X", "user_name", "cvtd_timestamp", "raw_timestamp_part_1", "raw_timestamp_part_2")
remove2 <- is.element(names(RawData), noInfoVariables)
remove <- !remove1&!remove2
ModelData <- RawData[remove]
inTrain <- createDataPartition(y=ModelData$classe, p=0.1, list=FALSE)
training <- ModelData[inTrain,]
testing <- ModelData[-inTrain,]
dim(training)
modelFit <- train(classe~., data = training, method = "rf")
